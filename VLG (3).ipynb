{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxF2Ss0IbyPA",
    "outputId": "7947204a-e719-412b-e673-f1b6d89e4817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "!pip install utils\n",
    "import utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fGk9u-UCcrzz"
   },
   "outputs": [],
   "source": [
    "train_sample=pd.read_csv('train.csv')\n",
    "test_sample=pd.read_csv('test.csv')\n",
    "solution = pd.read_csv('solution_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.033875</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-1.470684</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>-0.196530</td>\n",
       "      <td>-0.125239</td>\n",
       "      <td>-0.452284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111266</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.060039</td>\n",
       "      <td>0.301279</td>\n",
       "      <td>-1.174846</td>\n",
       "      <td>-1.076498</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-2.179176</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.348835</td>\n",
       "      <td>0.294815</td>\n",
       "      <td>-0.557577</td>\n",
       "      <td>-2.020773</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>1.633930</td>\n",
       "      <td>-1.680658</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735240</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>1.521941</td>\n",
       "      <td>1.347946</td>\n",
       "      <td>0.754505</td>\n",
       "      <td>1.330642</td>\n",
       "      <td>-0.754453</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.252671</td>\n",
       "      <td>1.495870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113248</td>\n",
       "      <td>-0.607726</td>\n",
       "      <td>-0.947791</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>-1.493958</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>-1.311018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>-1.035953</td>\n",
       "      <td>2.111387</td>\n",
       "      <td>-0.984415</td>\n",
       "      <td>1.148076</td>\n",
       "      <td>-1.433554</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.170083</td>\n",
       "      <td>1.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.223321</td>\n",
       "      <td>-0.479048</td>\n",
       "      <td>-1.925789</td>\n",
       "      <td>1.680377</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>-1.453307</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>1.065448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>-1.957863</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>1.505329</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-1.769443</td>\n",
       "      <td>-0.547756</td>\n",
       "      <td>-0.568122</td>\n",
       "      <td>0.244645</td>\n",
       "      <td>0.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.422684</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.227744</td>\n",
       "      <td>0.432854</td>\n",
       "      <td>0.608348</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>-0.538868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>1.441766</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.994721</td>\n",
       "      <td>1.143999</td>\n",
       "      <td>-2.166923</td>\n",
       "      <td>-1.199248</td>\n",
       "      <td>-1.028636</td>\n",
       "      <td>0.752791</td>\n",
       "      <td>0.317169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.526642</td>\n",
       "      <td>-0.590432</td>\n",
       "      <td>0.328210</td>\n",
       "      <td>-0.547433</td>\n",
       "      <td>-0.692556</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.539330</td>\n",
       "      <td>0.578212</td>\n",
       "      <td>0.547327</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131889</td>\n",
       "      <td>0.250693</td>\n",
       "      <td>-0.215257</td>\n",
       "      <td>0.236730</td>\n",
       "      <td>0.905414</td>\n",
       "      <td>-0.102015</td>\n",
       "      <td>-2.453250</td>\n",
       "      <td>-1.562715</td>\n",
       "      <td>-0.324501</td>\n",
       "      <td>1.590111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.199279</td>\n",
       "      <td>0.831787</td>\n",
       "      <td>-1.052687</td>\n",
       "      <td>0.491722</td>\n",
       "      <td>-0.571469</td>\n",
       "      <td>-1.449519</td>\n",
       "      <td>-0.227167</td>\n",
       "      <td>0.451431</td>\n",
       "      <td>0.090848</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081787</td>\n",
       "      <td>0.875338</td>\n",
       "      <td>-1.217201</td>\n",
       "      <td>-0.297249</td>\n",
       "      <td>-0.133911</td>\n",
       "      <td>-0.865070</td>\n",
       "      <td>-0.128914</td>\n",
       "      <td>1.153154</td>\n",
       "      <td>0.363864</td>\n",
       "      <td>0.670339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.906233</td>\n",
       "      <td>0.987997</td>\n",
       "      <td>-2.056229</td>\n",
       "      <td>1.078932</td>\n",
       "      <td>0.501608</td>\n",
       "      <td>0.826041</td>\n",
       "      <td>-0.136533</td>\n",
       "      <td>-0.931548</td>\n",
       "      <td>-0.657145</td>\n",
       "      <td>...</td>\n",
       "      <td>1.041859</td>\n",
       "      <td>-0.313667</td>\n",
       "      <td>1.492701</td>\n",
       "      <td>-0.425727</td>\n",
       "      <td>-0.425587</td>\n",
       "      <td>-0.408214</td>\n",
       "      <td>2.041689</td>\n",
       "      <td>0.814182</td>\n",
       "      <td>0.211569</td>\n",
       "      <td>0.343013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.283658</td>\n",
       "      <td>0.964051</td>\n",
       "      <td>-0.349123</td>\n",
       "      <td>-0.162445</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.501472</td>\n",
       "      <td>0.413006</td>\n",
       "      <td>-1.183602</td>\n",
       "      <td>-0.221886</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.612677</td>\n",
       "      <td>-0.714356</td>\n",
       "      <td>-0.033445</td>\n",
       "      <td>-2.350722</td>\n",
       "      <td>-1.373337</td>\n",
       "      <td>-1.521727</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>0.114279</td>\n",
       "      <td>1.320421</td>\n",
       "      <td>0.979955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.382773</td>\n",
       "      <td>-2.415394</td>\n",
       "      <td>0.969006</td>\n",
       "      <td>-1.690371</td>\n",
       "      <td>1.509442</td>\n",
       "      <td>0.914959</td>\n",
       "      <td>-0.161389</td>\n",
       "      <td>0.952598</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875176</td>\n",
       "      <td>-0.855081</td>\n",
       "      <td>0.318588</td>\n",
       "      <td>0.252447</td>\n",
       "      <td>0.400645</td>\n",
       "      <td>1.732217</td>\n",
       "      <td>-0.390191</td>\n",
       "      <td>0.628386</td>\n",
       "      <td>-1.518295</td>\n",
       "      <td>-0.218986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels       f_0       f_1       f_2       f_3       f_4       f_5  \\\n",
       "0       0 -2.033875  0.978446 -0.142131 -0.177117 -1.470684  1.669562   \n",
       "1       1 -0.348835  0.294815 -0.557577 -2.020773 -1.234715  1.633930   \n",
       "2       1  0.113248 -0.607726 -0.947791  0.830851  0.998291  0.498321   \n",
       "3       0  1.223321 -0.479048 -1.925789  1.680377  0.021840 -1.453307   \n",
       "4       0  0.160109  0.422684 -0.308029  0.227744  0.432854  0.608348   \n",
       "5       0 -0.526642 -0.590432  0.328210 -0.547433 -0.692556  0.175954   \n",
       "6       0 -0.199279  0.831787 -1.052687  0.491722 -0.571469 -1.449519   \n",
       "7       0 -0.906233  0.987997 -2.056229  1.078932  0.501608  0.826041   \n",
       "8       0 -0.283658  0.964051 -0.349123 -0.162445  0.238172  0.501472   \n",
       "9       0  1.382773 -2.415394  0.969006 -1.690371  1.509442  0.914959   \n",
       "\n",
       "        f_6       f_7       f_8  ...    f_1190    f_1191    f_1192    f_1193  \\\n",
       "0 -0.196530 -0.125239 -0.452284  ... -1.111266  0.716084  0.060039  0.301279   \n",
       "1 -1.680658 -0.358146  0.166122  ...  0.735240  0.829781  1.521941  1.347946   \n",
       "2 -1.493958  0.789572 -1.311018  ...  0.104698  0.616189 -1.035953  2.111387   \n",
       "3  0.605559 -0.019024  1.065448  ...  0.360237 -1.957863 -0.123384  1.505329   \n",
       "4  0.193832  1.035091 -0.538868  ...  0.416629  1.441766  0.212572 -0.994721   \n",
       "5  0.539330  0.578212  0.547327  ...  1.131889  0.250693 -0.215257  0.236730   \n",
       "6 -0.227167  0.451431  0.090848  ...  1.081787  0.875338 -1.217201 -0.297249   \n",
       "7 -0.136533 -0.931548 -0.657145  ...  1.041859 -0.313667  1.492701 -0.425727   \n",
       "8  0.413006 -1.183602 -0.221886  ... -1.612677 -0.714356 -0.033445 -2.350722   \n",
       "9 -0.161389  0.952598 -0.044989  ...  0.875176 -0.855081  0.318588  0.252447   \n",
       "\n",
       "     f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0 -1.174846 -1.076498 -0.069452 -0.604012 -2.179176  0.558003  \n",
       "1  0.754505  1.330642 -0.754453  0.582956  0.252671  1.495870  \n",
       "2 -0.984415  1.148076 -1.433554  0.243372  0.170083  1.274795  \n",
       "3  0.660290 -1.769443 -0.547756 -0.568122  0.244645  0.982116  \n",
       "4  1.143999 -2.166923 -1.199248 -1.028636  0.752791  0.317169  \n",
       "5  0.905414 -0.102015 -2.453250 -1.562715 -0.324501  1.590111  \n",
       "6 -0.133911 -0.865070 -0.128914  1.153154  0.363864  0.670339  \n",
       "7 -0.425587 -0.408214  2.041689  0.814182  0.211569  0.343013  \n",
       "8 -1.373337 -1.521727  0.015327  0.114279  1.320421  0.979955  \n",
       "9  0.400645  1.732217 -0.390191  0.628386 -1.518295 -0.218986  \n",
       "\n",
       "[10 rows x 1201 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_sample.drop('labels',axis ='columns')\n",
    "Y_train=train_sample['labels']\n",
    "X_test = test_sample.drop('id',axis ='columns')\n",
    "Y_test = solution['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.033875</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-1.470684</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>-0.196530</td>\n",
       "      <td>-0.125239</td>\n",
       "      <td>-0.452284</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111266</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.060039</td>\n",
       "      <td>0.301279</td>\n",
       "      <td>-1.174846</td>\n",
       "      <td>-1.076498</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-2.179176</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.348835</td>\n",
       "      <td>0.294815</td>\n",
       "      <td>-0.557577</td>\n",
       "      <td>-2.020773</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>1.633930</td>\n",
       "      <td>-1.680658</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>-1.656990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735240</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>1.521941</td>\n",
       "      <td>1.347946</td>\n",
       "      <td>0.754505</td>\n",
       "      <td>1.330642</td>\n",
       "      <td>-0.754453</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.252671</td>\n",
       "      <td>1.495870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113248</td>\n",
       "      <td>-0.607726</td>\n",
       "      <td>-0.947791</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>-1.493958</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>-1.311018</td>\n",
       "      <td>0.848524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>-1.035953</td>\n",
       "      <td>2.111387</td>\n",
       "      <td>-0.984415</td>\n",
       "      <td>1.148076</td>\n",
       "      <td>-1.433554</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.170083</td>\n",
       "      <td>1.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.223321</td>\n",
       "      <td>-0.479048</td>\n",
       "      <td>-1.925789</td>\n",
       "      <td>1.680377</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>-1.453307</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>1.065448</td>\n",
       "      <td>0.717341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>-1.957863</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>1.505329</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-1.769443</td>\n",
       "      <td>-0.547756</td>\n",
       "      <td>-0.568122</td>\n",
       "      <td>0.244645</td>\n",
       "      <td>0.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.422684</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.227744</td>\n",
       "      <td>0.432854</td>\n",
       "      <td>0.608348</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>-0.538868</td>\n",
       "      <td>0.778445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>1.441766</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.994721</td>\n",
       "      <td>1.143999</td>\n",
       "      <td>-2.166923</td>\n",
       "      <td>-1.199248</td>\n",
       "      <td>-1.028636</td>\n",
       "      <td>0.752791</td>\n",
       "      <td>0.317169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0 -2.033875  0.978446 -0.142131 -0.177117 -1.470684  1.669562 -0.196530   \n",
       "1 -0.348835  0.294815 -0.557577 -2.020773 -1.234715  1.633930 -1.680658   \n",
       "2  0.113248 -0.607726 -0.947791  0.830851  0.998291  0.498321 -1.493958   \n",
       "3  1.223321 -0.479048 -1.925789  1.680377  0.021840 -1.453307  0.605559   \n",
       "4  0.160109  0.422684 -0.308029  0.227744  0.432854  0.608348  0.193832   \n",
       "\n",
       "        f_7       f_8       f_9  ...    f_1190    f_1191    f_1192    f_1193  \\\n",
       "0 -0.125239 -0.452284 -0.128052  ... -1.111266  0.716084  0.060039  0.301279   \n",
       "1 -0.358146  0.166122 -1.656990  ...  0.735240  0.829781  1.521941  1.347946   \n",
       "2  0.789572 -1.311018  0.848524  ...  0.104698  0.616189 -1.035953  2.111387   \n",
       "3 -0.019024  1.065448  0.717341  ...  0.360237 -1.957863 -0.123384  1.505329   \n",
       "4  1.035091 -0.538868  0.778445  ...  0.416629  1.441766  0.212572 -0.994721   \n",
       "\n",
       "     f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0 -1.174846 -1.076498 -0.069452 -0.604012 -2.179176  0.558003  \n",
       "1  0.754505  1.330642 -0.754453  0.582956  0.252671  1.495870  \n",
       "2 -0.984415  1.148076 -1.433554  0.243372  0.170083  1.274795  \n",
       "3  0.660290 -1.769443 -0.547756 -0.568122  0.244645  0.982116  \n",
       "4  1.143999 -2.166923 -1.199248 -1.028636  0.752791  0.317169  \n",
       "\n",
       "[5 rows x 1200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v1qtgfL3dsX8"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(len(X_train),20,20,3)\n",
    "\n",
    "Y_train = Y_train.values.reshape(len(Y_train), 1)\n",
    "\n",
    "X_test = X_test.values.reshape(len(X_test), 20, 20, 3)\n",
    "Y_test = Y_test.values.reshape(len(Y_test), 1)\n",
    "\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (5250, 20, 20, 3)\n",
      "Shape of Y_train:  (5250, 1)\n",
      "Shape of X_test:  (2250, 20, 20, 3)\n",
      "Shape of Y_test:  (2250, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of Y_train: \", Y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of Y_test: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smOcdzi_d5-e",
    "outputId": "23a313e8-207b-490d-ff3d-7b2b2bb1c284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "165/165 [==============================] - 2s 8ms/step - loss: 2.1529 - accuracy: 0.7326\n",
      "Epoch 2/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.5296 - accuracy: 0.7446\n",
      "Epoch 3/10\n",
      "165/165 [==============================] - 1s 6ms/step - loss: 0.4457 - accuracy: 0.8194\n",
      "Epoch 4/10\n",
      "165/165 [==============================] - 1s 6ms/step - loss: 0.4134 - accuracy: 0.8446\n",
      "Epoch 5/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.3942 - accuracy: 0.8512\n",
      "Epoch 6/10\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.3735 - accuracy: 0.8539\n",
      "Epoch 7/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.3569 - accuracy: 0.8655\n",
      "Epoch 8/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.3402 - accuracy: 0.8726\n",
      "Epoch 9/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.3252 - accuracy: 0.8790\n",
      "Epoch 10/10\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.3217 - accuracy: 0.8838\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 2.2303 - accuracy: 0.5027\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "        layers.Conv2D(filters=16,kernel_size=(11,11),activation='relu',input_shape=(20,20,3),strides=(2,2),use_bias=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.MaxPool2D((2,2),padding='same'),\n",
    "        layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',use_bias=True),\n",
    "        layers.MaxPool2D((2,2),padding ='same'),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(l=0.01)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(32,activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(2,activation='softmax')\n",
    "        ])\n",
    "\n",
    "opt =tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',]\n",
    "    )\n",
    "model.fit(X_train,Y_train,epochs=10)\n",
    "accuracy_mod=model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "SFMGlCjUjQQb",
    "outputId": "d4014ed6-5b22-4730-c26f-0585c62b7589"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_model=XGBClassifier(n_estimators=1000)\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "xg_model.fit(X_train_flatten,Y_train)\n",
    "xg_model.fit(X_train_flatten,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtz2gOQ5kP7J",
    "outputId": "2b863673-5e48-4bd5-bcfd-906e2f96cb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.5111111111111111\n"
     ]
    }
   ],
   "source": [
    "predictions = xg_model.predict(X_test_flatten)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCLFOxoykS-8",
    "outputId": "d581dee0-66a4-4c23-e5b4-74fffe3cdf7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.82      0.61      1085\n",
      "           1       0.52      0.18      0.27      1165\n",
      "\n",
      "    accuracy                           0.49      2250\n",
      "   macro avg       0.50      0.50      0.44      2250\n",
      "weighted avg       0.50      0.49      0.43      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score=(Y_test,predictions)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
